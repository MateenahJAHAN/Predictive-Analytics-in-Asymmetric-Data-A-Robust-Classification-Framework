{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LoanTap Loan Default Prediction using Logistic Regression\n",
        "\n",
        "**Author:** Vidyasagar | Data Scientist  \n",
        "**Date:** February 2026  \n",
        "**Project:** Predictive Analytics in Asymmetric Data â€” A Robust Classification Framework\n",
        "\n",
        "---\n",
        "\n",
        "## Problem Statement\n",
        "\n",
        "LoanTap is an online platform committed to delivering customized loan products to millennials. They innovate in an otherwise dull loan segment, to deliver instant, flexible loans on consumer-friendly terms to salaried professionals and businessmen.\n",
        "\n",
        "The key business problem is to **predict whether a borrower will default on their loan** (Charged Off) or **fully pay** it back. This is critical because:\n",
        "\n",
        "1. **False Positives (predicting default when they won't):** The company loses business opportunity and potential interest income.\n",
        "2. **False Negatives (predicting fully paid when they will default):** The company faces Non-Performing Assets (NPA), leading to financial losses.\n",
        "\n",
        "We will build a **Logistic Regression** model to predict the `loan_status` (target variable) using various borrower attributes and loan characteristics.\n",
        "\n",
        "### Approach\n",
        "1. Exploratory Data Analysis (EDA)\n",
        "2. Feature Engineering\n",
        "3. Data Preprocessing (Missing values, Outliers, Scaling)\n",
        "4. Logistic Regression Model Building\n",
        "5. Model Evaluation (ROC-AUC, Precision-Recall, Classification Report)\n",
        "6. Precision vs Recall Tradeoff Analysis\n",
        "7. Actionable Insights & Recommendations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 1. Import Libraries & Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# Import Required Libraries\n",
        "# ============================================================\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import (\n",
        "    classification_report, confusion_matrix, roc_auc_score,\n",
        "    roc_curve, precision_recall_curve, auc, accuracy_score,\n",
        "    precision_score, recall_score, f1_score\n",
        ")\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# Plot settings\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "plt.rcParams['font.size'] = 12\n",
        "plt.rcParams['axes.titlesize'] = 14\n",
        "plt.rcParams['axes.labelsize'] = 12\n",
        "sns.set_style('whitegrid')\n",
        "sns.set_palette('husl')\n",
        "\n",
        "print(\"All libraries imported successfully!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# Load the Dataset\n",
        "# ============================================================\n",
        "df = pd.read_csv('../data/LoanTapData.csv')\n",
        "print(f\"Dataset Shape: {df.shape}\")\n",
        "print(f\"Number of Records: {df.shape[0]}\")\n",
        "print(f\"Number of Features: {df.shape[1]}\")\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 2. Exploratory Data Analysis (EDA)\n",
        "\n",
        "### 2.1 Data Structure & Characteristics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# 2.1.1 Dataset Info & Data Types\n",
        "# ============================================================\n",
        "print(\"=\" * 60)\n",
        "print(\"DATASET INFORMATION\")\n",
        "print(\"=\" * 60)\n",
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# 2.1.2 Statistical Summary of Numerical Columns\n",
        "# ============================================================\n",
        "print(\"=\" * 60)\n",
        "print(\"STATISTICAL SUMMARY - NUMERICAL FEATURES\")\n",
        "print(\"=\" * 60)\n",
        "df.describe().T.round(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# 2.1.3 Statistical Summary of Categorical Columns\n",
        "# ============================================================\n",
        "print(\"=\" * 60)\n",
        "print(\"STATISTICAL SUMMARY - CATEGORICAL FEATURES\")\n",
        "print(\"=\" * 60)\n",
        "df.describe(include='object').T"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# 2.1.4 Missing Values Analysis\n",
        "# ============================================================\n",
        "print(\"=\" * 60)\n",
        "print(\"MISSING VALUES ANALYSIS\")\n",
        "print(\"=\" * 60)\n",
        "missing = df.isnull().sum()\n",
        "missing_pct = (df.isnull().sum() / len(df)) * 100\n",
        "missing_df = pd.DataFrame({'Missing Count': missing, 'Missing %': missing_pct.round(2)})\n",
        "missing_df = missing_df[missing_df['Missing Count'] > 0].sort_values('Missing %', ascending=False)\n",
        "print(missing_df)\n",
        "print(f\"\\nTotal columns with missing values: {len(missing_df)}\")\n",
        "\n",
        "# Visualize missing values\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "missing_df['Missing %'].plot(kind='barh', color=sns.color_palette('RdYlGn_r', len(missing_df)), ax=ax)\n",
        "ax.set_title('Missing Values Percentage by Feature', fontsize=16, fontweight='bold')\n",
        "ax.set_xlabel('Missing Percentage (%)')\n",
        "for i, v in enumerate(missing_df['Missing %']):\n",
        "    ax.text(v + 0.1, i, f'{v:.2f}%', va='center', fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig('../figures/missing_values.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(\"\\n>> Insight: mort_acc has the highest missing percentage (~8%). emp_title (~4%) and emp_length (~3%) also have notable missing values.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# 2.1.5 Duplicate Values Check\n",
        "# ============================================================\n",
        "print(\"=\" * 60)\n",
        "print(\"DUPLICATE VALUES CHECK\")\n",
        "print(\"=\" * 60)\n",
        "duplicates = df.duplicated().sum()\n",
        "print(f\"Number of duplicate rows: {duplicates}\")\n",
        "print(f\"Percentage of duplicates: {(duplicates/len(df))*100:.2f}%\")\n",
        "if duplicates > 0:\n",
        "    df.drop_duplicates(inplace=True)\n",
        "    print(f\"Duplicates removed. New shape: {df.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Target Variable Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# 2.2.1 Target Variable Distribution\n",
        "# ============================================================\n",
        "print(\"=\" * 60)\n",
        "print(\"TARGET VARIABLE - LOAN STATUS DISTRIBUTION\")\n",
        "print(\"=\" * 60)\n",
        "target_counts = df['loan_status'].value_counts()\n",
        "target_pct = df['loan_status'].value_counts(normalize=True) * 100\n",
        "\n",
        "print(f\"\\nFully Paid: {target_counts['Fully Paid']} ({target_pct['Fully Paid']:.2f}%)\")\n",
        "print(f\"Charged Off: {target_counts['Charged Off']} ({target_pct['Charged Off']:.2f}%)\")\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "# Count Plot\n",
        "colors = ['#2ecc71', '#e74c3c']\n",
        "sns.countplot(data=df, x='loan_status', palette=colors, ax=axes[0], edgecolor='black')\n",
        "axes[0].set_title('Loan Status Distribution', fontsize=16, fontweight='bold')\n",
        "axes[0].set_xlabel('Loan Status')\n",
        "axes[0].set_ylabel('Count')\n",
        "for p in axes[0].patches:\n",
        "    axes[0].annotate(f'{int(p.get_height())}\\n({p.get_height()/len(df)*100:.1f}%)', \n",
        "                     (p.get_x() + p.get_width()/2., p.get_height()),\n",
        "                     ha='center', va='bottom', fontweight='bold', fontsize=13)\n",
        "\n",
        "# Pie Chart\n",
        "axes[1].pie(target_counts, labels=target_counts.index, autopct='%1.1f%%',\n",
        "            colors=colors, startangle=90, explode=(0.05, 0.05),\n",
        "            shadow=True, textprops={'fontsize': 13, 'fontweight': 'bold'})\n",
        "axes[1].set_title('Loan Status Proportion', fontsize=16, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('../figures/target_distribution.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\n>> ANSWER Q1: {target_pct['Fully Paid']:.2f}% of customers have fully paid their loan amount.\")\n",
        "print(\">> Insight: The dataset is imbalanced with ~76% Fully Paid and ~24% Charged Off loans.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.3 Univariate Analysis - Continuous Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# 2.3.1 Distribution of Continuous Variables\n",
        "# ============================================================\n",
        "continuous_cols = ['loan_amnt', 'int_rate', 'installment', 'annual_inc', 'dti', \n",
        "                   'open_acc', 'revol_bal', 'revol_util', 'total_acc']\n",
        "\n",
        "fig, axes = plt.subplots(3, 3, figsize=(20, 18))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, col in enumerate(continuous_cols):\n",
        "    ax = axes[idx]\n",
        "    sns.histplot(df[col].dropna(), kde=True, bins=40, color=sns.color_palette('husl', 9)[idx], ax=ax, edgecolor='black', alpha=0.7)\n",
        "    ax.axvline(df[col].mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {df[col].mean():.2f}')\n",
        "    ax.axvline(df[col].median(), color='blue', linestyle='-.', linewidth=2, label=f'Median: {df[col].median():.2f}')\n",
        "    ax.set_title(f'Distribution of {col}', fontsize=14, fontweight='bold')\n",
        "    ax.legend(fontsize=9)\n",
        "\n",
        "plt.suptitle('Univariate Analysis - Distribution of Continuous Variables', fontsize=18, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.savefig('../figures/univariate_continuous.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\">> Insights:\")\n",
        "print(\"   - loan_amnt: Right-skewed distribution with most loans between $5,000 - $20,000\")\n",
        "print(\"   - int_rate: Approximately normal distribution centered around 12-13%\")\n",
        "print(\"   - annual_inc: Heavily right-skewed with most incomes between $30,000 - $100,000\")\n",
        "print(\"   - dti: Right-skewed; most borrowers have DTI between 10-25\")\n",
        "print(\"   - revol_bal: Highly right-skewed indicating few borrowers with very high revolving balances\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# 2.3.2 Box Plots for Outlier Detection\n",
        "# ============================================================\n",
        "fig, axes = plt.subplots(3, 3, figsize=(20, 18))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, col in enumerate(continuous_cols):\n",
        "    ax = axes[idx]\n",
        "    sns.boxplot(data=df, y=col, ax=ax, color=sns.color_palette('Set2')[idx % 8], width=0.5)\n",
        "    \n",
        "    # Calculate IQR\n",
        "    Q1 = df[col].quantile(0.25)\n",
        "    Q3 = df[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    outliers = ((df[col] < Q1 - 1.5 * IQR) | (df[col] > Q3 + 1.5 * IQR)).sum()\n",
        "    \n",
        "    ax.set_title(f'{col}\\n(Outliers: {outliers})', fontsize=13, fontweight='bold')\n",
        "\n",
        "plt.suptitle('Box Plots - Outlier Detection in Continuous Variables', fontsize=18, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.savefig('../figures/boxplots_outliers.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\">> Insights on Outliers:\")\n",
        "print(\"   - annual_inc has significant outliers (very high income earners)\")\n",
        "print(\"   - revol_bal shows extreme outliers on the higher end\")\n",
        "print(\"   - open_acc and total_acc have moderate outliers\")\n",
        "print(\"   - These outliers need treatment before model building\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.4 Univariate Analysis - Categorical Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# 2.4.1 Categorical Variable Distributions\n",
        "# ============================================================\n",
        "cat_cols = ['term', 'grade', 'home_ownership', 'verification_status', 'purpose', 'initial_list_status', 'application_type']\n",
        "\n",
        "fig, axes = plt.subplots(4, 2, figsize=(18, 24))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, col in enumerate(cat_cols):\n",
        "    ax = axes[idx]\n",
        "    order = df[col].value_counts().index\n",
        "    sns.countplot(data=df, x=col, order=order, palette='viridis', ax=ax, edgecolor='black')\n",
        "    ax.set_title(f'Distribution of {col}', fontsize=14, fontweight='bold')\n",
        "    ax.tick_params(axis='x', rotation=45)\n",
        "    for p in ax.patches:\n",
        "        ax.annotate(f'{int(p.get_height())}', \n",
        "                     (p.get_x() + p.get_width()/2., p.get_height()),\n",
        "                     ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
        "\n",
        "# Remove empty subplot\n",
        "axes[-1].set_visible(False)\n",
        "\n",
        "plt.suptitle('Univariate Analysis - Categorical Variables', fontsize=18, fontweight='bold', y=1.01)\n",
        "plt.tight_layout()\n",
        "plt.savefig('../figures/univariate_categorical.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\">> Insights:\")\n",
        "print(f\"   - ANSWER Q3: The majority of people have home ownership as '{df['home_ownership'].value_counts().index[0]}'\")\n",
        "print(\"   - 36-month term loans are significantly more popular than 60-month\")\n",
        "print(\"   - Grades B and C are the most common loan grades\")\n",
        "print(\"   - debt_consolidation is the dominant purpose for loans\")\n",
        "print(\"   - Individual applications far outnumber joint applications\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# 2.4.2 Employment Title Analysis (Top 20)\n",
        "# ============================================================\n",
        "fig, ax = plt.subplots(figsize=(14, 8))\n",
        "top_titles = df['emp_title'].value_counts().head(20)\n",
        "top_titles.plot(kind='barh', color=sns.color_palette('coolwarm', 20), ax=ax, edgecolor='black')\n",
        "ax.set_title('Top 20 Employment Titles', fontsize=16, fontweight='bold')\n",
        "ax.set_xlabel('Count')\n",
        "ax.set_ylabel('Job Title')\n",
        "for i, v in enumerate(top_titles):\n",
        "    ax.text(v + 5, i, str(v), va='center', fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig('../figures/top_emp_titles.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "top2 = df['emp_title'].value_counts().head(2).index.tolist()\n",
        "print(f\"\\n>> ANSWER Q5: The top 2 most afforded job titles are: '{top2[0]}' and '{top2[1]}'\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.5 Bivariate Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# 2.5.1 Loan Status vs Numerical Features\n",
        "# ============================================================\n",
        "fig, axes = plt.subplots(3, 3, figsize=(20, 18))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, col in enumerate(continuous_cols):\n",
        "    ax = axes[idx]\n",
        "    sns.boxplot(data=df, x='loan_status', y=col, palette=['#2ecc71', '#e74c3c'], ax=ax)\n",
        "    ax.set_title(f'Loan Status vs {col}', fontsize=13, fontweight='bold')\n",
        "\n",
        "plt.suptitle('Bivariate Analysis - Loan Status vs Numerical Features', fontsize=18, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.savefig('../figures/bivariate_numerical.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\">> Insights:\")\n",
        "print(\"   - Charged Off loans tend to have higher interest rates\")\n",
        "print(\"   - Higher loan amounts are slightly more associated with defaults\")\n",
        "print(\"   - DTI ratio is slightly higher for charged off loans\")\n",
        "print(\"   - Annual income doesn't show a dramatic difference between groups\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# 2.5.2 Loan Status vs Categorical Features\n",
        "# ============================================================\n",
        "fig, axes = plt.subplots(3, 2, figsize=(18, 20))\n",
        "axes = axes.flatten()\n",
        "\n",
        "cat_bi_cols = ['grade', 'term', 'home_ownership', 'verification_status', 'purpose', 'emp_length']\n",
        "\n",
        "for idx, col in enumerate(cat_bi_cols):\n",
        "    ax = axes[idx]\n",
        "    ct = pd.crosstab(df[col], df['loan_status'], normalize='index') * 100\n",
        "    ct.plot(kind='bar', stacked=True, color=['#e74c3c', '#2ecc71'], ax=ax, edgecolor='black')\n",
        "    ax.set_title(f'Loan Status by {col}', fontsize=14, fontweight='bold')\n",
        "    ax.set_ylabel('Percentage (%)')\n",
        "    ax.tick_params(axis='x', rotation=45)\n",
        "    ax.legend(title='Loan Status', loc='upper right')\n",
        "\n",
        "plt.suptitle('Bivariate Analysis - Loan Status vs Categorical Features', fontsize=18, fontweight='bold', y=1.01)\n",
        "plt.tight_layout()\n",
        "plt.savefig('../figures/bivariate_categorical.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Check Grade A performance\n",
        "grade_a_fullypaid = pd.crosstab(df['grade'], df['loan_status'], normalize='index')['Fully Paid']['A'] * 100\n",
        "print(f\"\\n>> ANSWER Q4: People with grade 'A' fully pay {grade_a_fullypaid:.1f}% of the time.\")\n",
        "print(f\"   People with grades 'A' are more likely to fully pay their loan = TRUE\")\n",
        "print(\"\\n>> Insights:\")\n",
        "print(\"   - Clear gradient: Higher grade (A) = lower default rate, Lower grade (G) = higher default rate\")\n",
        "print(\"   - 60-month term loans have higher default rates than 36-month\")\n",
        "print(\"   - Home ownership category doesn't dramatically affect default rates\")\n",
        "print(\"   - Small business loans have the highest default rates among purposes\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# 2.5.3 Correlation Analysis\n",
        "# ============================================================\n",
        "numerical_df = df[continuous_cols + ['pub_rec', 'mort_acc', 'pub_rec_bankruptcies']].copy()\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(14, 10))\n",
        "corr_matrix = numerical_df.corr()\n",
        "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
        "sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.2f', cmap='RdYlBu_r',\n",
        "            center=0, square=True, linewidths=1, ax=ax,\n",
        "            cbar_kws={'shrink': 0.8})\n",
        "ax.set_title('Correlation Heatmap - Numerical Features', fontsize=16, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig('../figures/correlation_heatmap.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Loan Amount vs Installment correlation\n",
        "corr_loan_inst = df['loan_amnt'].corr(df['installment'])\n",
        "print(f\"\\n>> ANSWER Q2: Correlation between Loan Amount and Installment = {corr_loan_inst:.4f}\")\n",
        "print(f\"   There is a very strong positive correlation between loan_amnt and installment.\")\n",
        "print(f\"   This makes sense because installment is directly calculated from the loan amount.\")\n",
        "print(\"\\n>> Other Insights:\")\n",
        "print(\"   - loan_amnt and installment are highly correlated (multicollinearity concern)\")\n",
        "print(\"   - open_acc and total_acc show moderate positive correlation\")\n",
        "print(\"   - pub_rec and pub_rec_bankruptcies are positively correlated\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# 2.5.4 Scatter Plot: Loan Amount vs Installment\n",
        "# ============================================================\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "colors_map = {'Fully Paid': '#2ecc71', 'Charged Off': '#e74c3c'}\n",
        "for status in ['Fully Paid', 'Charged Off']:\n",
        "    mask = df['loan_status'] == status\n",
        "    ax.scatter(df.loc[mask, 'loan_amnt'], df.loc[mask, 'installment'], \n",
        "               alpha=0.3, s=10, c=colors_map[status], label=status)\n",
        "ax.set_title('Loan Amount vs Installment (colored by Loan Status)', fontsize=16, fontweight='bold')\n",
        "ax.set_xlabel('Loan Amount ($)')\n",
        "ax.set_ylabel('Installment ($)')\n",
        "ax.legend(markerscale=5)\n",
        "plt.tight_layout()\n",
        "plt.savefig('../figures/scatter_loan_installment.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\">> Insight: The strong linear relationship between loan_amnt and installment is clearly visible.\")\n",
        "print(\"   Two distinct bands correspond to 36-month and 60-month terms.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 3. Data Preprocessing\n",
        "\n",
        "### 3.1 Feature Engineering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# 3.1.1 Create a working copy\n",
        "# ============================================================\n",
        "df_processed = df.copy()\n",
        "print(f\"Working dataset shape: {df_processed.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# 3.1.2 Feature Engineering - Create Flag Variables\n",
        "# ============================================================\n",
        "\n",
        "# Flag: pub_rec > 0 then 1 else 0\n",
        "df_processed['pub_rec_flag'] = (df_processed['pub_rec'] > 0).astype(int)\n",
        "\n",
        "# Flag: mort_acc > 0 then 1 else 0  \n",
        "df_processed['mort_acc_flag'] = (df_processed['mort_acc'] > 0).astype(int)\n",
        "\n",
        "# Flag: pub_rec_bankruptcies > 0 then 1 else 0\n",
        "df_processed['pub_rec_bankruptcies_flag'] = (df_processed['pub_rec_bankruptcies'] > 0).astype(int)\n",
        "\n",
        "print(\"Flag variables created:\")\n",
        "print(f\"  pub_rec_flag distribution: \\n{df_processed['pub_rec_flag'].value_counts()}\")\n",
        "print(f\"\\n  mort_acc_flag distribution: \\n{df_processed['mort_acc_flag'].value_counts()}\")\n",
        "print(f\"\\n  pub_rec_bankruptcies_flag distribution: \\n{df_processed['pub_rec_bankruptcies_flag'].value_counts()}\")\n",
        "\n",
        "print(\"\\n>> Insight: These binary flags capture whether a borrower has ANY public records, mortgages, or bankruptcies.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# 3.1.3 Feature Engineering - Extract State from Address\n",
        "# ============================================================\n",
        "df_processed['state'] = df_processed['Address'].apply(lambda x: str(x).split('\\n')[-1].split()[0] if pd.notna(x) else np.nan)\n",
        "print(f\"Unique states: {df_processed['state'].nunique()}\")\n",
        "print(f\"\\nTop 10 states:\")\n",
        "print(df_processed['state'].value_counts().head(10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# 2.5.5 Geographic Analysis - Default Rates by State\n",
        "# ============================================================\n",
        "state_default = df_processed.groupby('state')['loan_status'].apply(\n",
        "    lambda x: (x == 'Charged Off').mean() * 100\n",
        ").sort_values(ascending=False)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(16, 8))\n",
        "state_default.plot(kind='bar', color=sns.color_palette('RdYlGn_r', len(state_default)), ax=ax, edgecolor='black')\n",
        "ax.set_title('Default Rate by State', fontsize=16, fontweight='bold')\n",
        "ax.set_ylabel('Default Rate (%)')\n",
        "ax.set_xlabel('State')\n",
        "ax.axhline(y=state_default.mean(), color='red', linestyle='--', linewidth=2, label=f'Average: {state_default.mean():.1f}%')\n",
        "ax.legend(fontsize=12)\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.savefig('../figures/default_by_state.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\n>> ANSWER Q9: Will the results be affected by geographical location? YES\")\n",
        "print(f\"   Default rates vary across states from {state_default.min():.1f}% to {state_default.max():.1f}%.\")\n",
        "print(f\"   This suggests geographical location does affect loan default behavior.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# 3.1.4 Feature Engineering - Term to numeric\n",
        "# ============================================================\n",
        "df_processed['term_numeric'] = df_processed['term'].apply(lambda x: int(str(x).strip().split()[0]))\n",
        "print(f\"Term numeric values: {df_processed['term_numeric'].unique()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# 3.1.5 Feature Engineering - Employment length to numeric\n",
        "# ============================================================\n",
        "def emp_length_to_num(x):\n",
        "    if pd.isna(x):\n",
        "        return np.nan\n",
        "    x = str(x).strip()\n",
        "    if '10+' in x:\n",
        "        return 10\n",
        "    elif '< 1' in x:\n",
        "        return 0\n",
        "    else:\n",
        "        try:\n",
        "            return int(x.split()[0])\n",
        "        except:\n",
        "            return np.nan\n",
        "\n",
        "df_processed['emp_length_numeric'] = df_processed['emp_length'].apply(emp_length_to_num)\n",
        "print(f\"Employment length numeric values: {sorted(df_processed['emp_length_numeric'].dropna().unique())}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# 3.1.6 Feature Engineering - Earliest credit line to years\n",
        "# ============================================================\n",
        "from datetime import datetime\n",
        "\n",
        "def credit_line_years(x):\n",
        "    if pd.isna(x):\n",
        "        return np.nan\n",
        "    try:\n",
        "        date = pd.to_datetime(x, format='%b-%Y')\n",
        "        return round((datetime(2020, 12, 1) - date).days / 365.25, 1)\n",
        "    except:\n",
        "        return np.nan\n",
        "\n",
        "df_processed['credit_history_years'] = df_processed['earliest_cr_line'].apply(credit_line_years)\n",
        "print(f\"Credit history years - Mean: {df_processed['credit_history_years'].mean():.1f}, Median: {df_processed['credit_history_years'].median():.1f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# 3.1.7 Encode Target Variable\n",
        "# ============================================================\n",
        "df_processed['target'] = (df_processed['loan_status'] == 'Charged Off').astype(int)\n",
        "print(f\"Target encoding: Charged Off = 1, Fully Paid = 0\")\n",
        "print(f\"Target distribution:\\n{df_processed['target'].value_counts()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Missing Value Treatment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# 3.2.1 Handle Missing Values\n",
        "# ============================================================\n",
        "print(\"Missing values BEFORE treatment:\")\n",
        "print(df_processed.isnull().sum()[df_processed.isnull().sum() > 0])\n",
        "\n",
        "# emp_title: Drop (too many unique values, not useful for modeling)\n",
        "# emp_length: Fill with median\n",
        "df_processed['emp_length_numeric'] = df_processed['emp_length_numeric'].fillna(df_processed['emp_length_numeric'].median())\n",
        "\n",
        "# title: Drop (redundant with purpose)\n",
        "# dti: Fill with median\n",
        "df_processed['dti'] = df_processed['dti'].fillna(df_processed['dti'].median())\n",
        "\n",
        "# revol_util: Fill with median\n",
        "df_processed['revol_util'] = df_processed['revol_util'].fillna(df_processed['revol_util'].median())\n",
        "\n",
        "# mort_acc: Fill with median based on total_acc groups\n",
        "total_acc_avg = df_processed.groupby('total_acc')['mort_acc'].transform('median')\n",
        "df_processed['mort_acc'] = df_processed['mort_acc'].fillna(total_acc_avg)\n",
        "df_processed['mort_acc'] = df_processed['mort_acc'].fillna(df_processed['mort_acc'].median())\n",
        "\n",
        "# pub_rec_bankruptcies: Fill with 0 (most common value)\n",
        "df_processed['pub_rec_bankruptcies'] = df_processed['pub_rec_bankruptcies'].fillna(0)\n",
        "\n",
        "# Update flags after filling\n",
        "df_processed['mort_acc_flag'] = (df_processed['mort_acc'] > 0).astype(int)\n",
        "df_processed['pub_rec_bankruptcies_flag'] = (df_processed['pub_rec_bankruptcies'] > 0).astype(int)\n",
        "\n",
        "# Fill any remaining NaN values in ALL numeric columns\n",
        "for col in df_processed.select_dtypes(include=['number']).columns:\n",
        "    if df_processed[col].isnull().sum() > 0:\n",
        "        df_processed[col] = df_processed[col].fillna(df_processed[col].median())\n",
        "\n",
        "print(\"\\nMissing values AFTER treatment:\")\n",
        "remaining_missing = df_processed.isnull().sum()[df_processed.isnull().sum() > 0]\n",
        "print(remaining_missing if len(remaining_missing) > 0 else \"No missing values in numerical features!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 Outlier Treatment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# 3.3.1 Outlier Treatment using IQR Capping\n",
        "# ============================================================\n",
        "def cap_outliers(df, col):\n",
        "    Q1 = df[col].quantile(0.01)\n",
        "    Q3 = df[col].quantile(0.99)\n",
        "    df[col] = df[col].clip(lower=Q1, upper=Q3)\n",
        "    return df\n",
        "\n",
        "outlier_cols = ['annual_inc', 'revol_bal', 'open_acc', 'total_acc', 'dti']\n",
        "\n",
        "print(\"Outlier treatment (1st-99th percentile capping):\")\n",
        "for col in outlier_cols:\n",
        "    before_min, before_max = df_processed[col].min(), df_processed[col].max()\n",
        "    df_processed = cap_outliers(df_processed, col)\n",
        "    after_min, after_max = df_processed[col].min(), df_processed[col].max()\n",
        "    print(f\"  {col}: [{before_min:.0f}, {before_max:.0f}] -> [{after_min:.0f}, {after_max:.0f}]\")\n",
        "\n",
        "print(\"\\n>> Insight: Outliers capped at 1st and 99th percentiles to reduce extreme values without losing too much information.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.4 Data Preparation for Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# 3.4.1 Select Features for Modeling\n",
        "# ============================================================\n",
        "\n",
        "# Drop columns not needed for modeling\n",
        "drop_cols = ['loan_status', 'emp_title', 'emp_length', 'title', 'issue_d', \n",
        "             'earliest_cr_line', 'Address', 'term', 'sub_grade', 'state']\n",
        "\n",
        "# Features to keep\n",
        "feature_cols = [col for col in df_processed.columns if col not in drop_cols + ['target']]\n",
        "print(f\"Features selected for modeling: {len(feature_cols)}\")\n",
        "print(f\"Features: {feature_cols}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# 3.4.2 Encode Categorical Variables\n",
        "# ============================================================\n",
        "# Identify remaining categorical columns\n",
        "cat_features = df_processed[feature_cols].select_dtypes(include=['object']).columns.tolist()\n",
        "num_features = df_processed[feature_cols].select_dtypes(include=['number']).columns.tolist()\n",
        "\n",
        "print(f\"Categorical features to encode: {cat_features}\")\n",
        "print(f\"Numerical features: {num_features}\")\n",
        "\n",
        "# One-hot encode categorical variables\n",
        "df_model = pd.get_dummies(df_processed[feature_cols + ['target']], columns=cat_features, drop_first=True)\n",
        "\n",
        "# Ensure no NaN values remain (fill any residual NaN with 0)\n",
        "df_model = df_model.fillna(0)\n",
        "\n",
        "# Convert all columns to float to avoid dtype issues\n",
        "for col in df_model.columns:\n",
        "    if col != 'target':\n",
        "        df_model[col] = df_model[col].astype(float)\n",
        "\n",
        "print(f\"\\nDataset shape after encoding: {df_model.shape}\")\n",
        "print(f\"Remaining NaN values: {df_model.isnull().sum().sum()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# 3.4.3 Train-Test Split\n",
        "# ============================================================\n",
        "X = df_model.drop('target', axis=1)\n",
        "y = df_model['target']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "\n",
        "print(f\"Training set: {X_train.shape[0]} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
        "print(f\"Test set: {X_test.shape[0]} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
        "print(f\"\\nTarget distribution in train: \\n{y_train.value_counts(normalize=True).round(4) * 100}\")\n",
        "print(f\"\\nTarget distribution in test: \\n{y_test.value_counts(normalize=True).round(4) * 100}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# 3.4.4 Feature Scaling using StandardScaler\n",
        "# ============================================================\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n",
        "X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns, index=X_test.index)\n",
        "\n",
        "print(\"Feature scaling applied using StandardScaler\")\n",
        "print(f\"\\nScaled Training data - Mean (should be ~0): {X_train_scaled.mean().mean():.6f}\")\n",
        "print(f\"Scaled Training data - Std (should be ~1): {X_train_scaled.std().mean():.6f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 4. Model Building\n",
        "\n",
        "### 4.1 Logistic Regression Model (Scikit-Learn)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# 4.1.1 Build Logistic Regression Model\n",
        "# ============================================================\n",
        "lr_model = LogisticRegression(random_state=42, max_iter=1000, class_weight='balanced', solver='liblinear')\n",
        "lr_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "print(\"Logistic Regression Model trained successfully!\")\n",
        "print(f\"\\nModel Parameters:\")\n",
        "print(f\"  Solver: liblinear\")\n",
        "print(f\"  Max iterations: 1000\")\n",
        "print(f\"  Class weight: balanced (handles class imbalance)\")\n",
        "print(f\"  Number of features: {X_train_scaled.shape[1]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# 4.1.2 Model Coefficients\n",
        "# ============================================================\n",
        "coefficients = pd.DataFrame({\n",
        "    'Feature': X_train_scaled.columns,\n",
        "    'Coefficient': lr_model.coef_[0],\n",
        "    'Abs_Coefficient': np.abs(lr_model.coef_[0])\n",
        "}).sort_values('Abs_Coefficient', ascending=False)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"MODEL COEFFICIENTS (sorted by absolute value)\")\n",
        "print(\"=\" * 60)\n",
        "print(coefficients[['Feature', 'Coefficient']].to_string(index=False))\n",
        "\n",
        "# Visualize top 20 coefficients\n",
        "top_coefs = coefficients.head(20)\n",
        "fig, ax = plt.subplots(figsize=(12, 10))\n",
        "colors = ['#e74c3c' if c > 0 else '#2ecc71' for c in top_coefs['Coefficient']]\n",
        "ax.barh(top_coefs['Feature'], top_coefs['Coefficient'], color=colors, edgecolor='black')\n",
        "ax.set_title('Top 20 Most Important Features (Logistic Regression Coefficients)', fontsize=16, fontweight='bold')\n",
        "ax.set_xlabel('Coefficient Value')\n",
        "ax.axvline(x=0, color='black', linewidth=1)\n",
        "plt.tight_layout()\n",
        "plt.savefig('../figures/feature_importance.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n>> ANSWER Q8: Features that heavily affected the outcome:\")\n",
        "for i, row in coefficients.head(10).iterrows():\n",
        "    direction = 'increases' if row['Coefficient'] > 0 else 'decreases'\n",
        "    print(f\"   - {row['Feature']}: coefficient = {row['Coefficient']:.4f} ({direction} default probability)\")\n",
        "\n",
        "print(f\"\\nIntercept: {lr_model.intercept_[0]:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# 4.1.3 Statsmodels Logistic Regression (for p-values)\n",
        "# ============================================================\n",
        "X_train_sm = sm.add_constant(X_train_scaled)\n",
        "logit_model = sm.Logit(y_train, X_train_sm)\n",
        "result = logit_model.fit(maxiter=1000, disp=0)\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"STATSMODELS LOGISTIC REGRESSION SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "print(result.summary2())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 5. Model Evaluation\n",
        "\n",
        "### 5.1 Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# 5.1.1 Generate Predictions\n",
        "# ============================================================\n",
        "y_pred = lr_model.predict(X_test_scaled)\n",
        "y_pred_proba = lr_model.predict_proba(X_test_scaled)[:, 1]\n",
        "\n",
        "print(f\"Predictions generated for {len(y_test)} test samples\")\n",
        "print(f\"\\nPredicted class distribution: \\n{pd.Series(y_pred).value_counts()}\")\n",
        "print(f\"\\nActual class distribution: \\n{y_test.value_counts()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 Classification Report & Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# 5.2.1 Classification Report\n",
        "# ============================================================\n",
        "print(\"=\" * 60)\n",
        "print(\"CLASSIFICATION REPORT\")\n",
        "print(\"=\" * 60)\n",
        "print(classification_report(y_test, y_pred, target_names=['Fully Paid (0)', 'Charged Off (1)']))\n",
        "\n",
        "print(f\"\\nOverall Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Precision (Charged Off): {precision_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Recall (Charged Off): {recall_score(y_test, y_pred):.4f}\")\n",
        "print(f\"F1-Score (Charged Off): {f1_score(y_test, y_pred):.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# 5.2.2 Confusion Matrix Visualization\n",
        "# ============================================================\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Raw counts\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
        "            xticklabels=['Fully Paid', 'Charged Off'],\n",
        "            yticklabels=['Fully Paid', 'Charged Off'])\n",
        "axes[0].set_title('Confusion Matrix (Counts)', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xlabel('Predicted')\n",
        "axes[0].set_ylabel('Actual')\n",
        "\n",
        "# Percentages\n",
        "cm_pct = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
        "sns.heatmap(cm_pct, annot=True, fmt='.1f', cmap='RdYlGn', ax=axes[1],\n",
        "            xticklabels=['Fully Paid', 'Charged Off'],\n",
        "            yticklabels=['Fully Paid', 'Charged Off'])\n",
        "axes[1].set_title('Confusion Matrix (Percentages)', fontsize=14, fontweight='bold')\n",
        "axes[1].set_xlabel('Predicted')\n",
        "axes[1].set_ylabel('Actual')\n",
        "\n",
        "plt.suptitle('Confusion Matrix Analysis', fontsize=16, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig('../figures/confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "print(f\"\\nTrue Negatives (Correctly predicted Fully Paid): {tn}\")\n",
        "print(f\"False Positives (Incorrectly predicted Charged Off): {fp}\")\n",
        "print(f\"False Negatives (Missed Defaults): {fn}\")\n",
        "print(f\"True Positives (Correctly predicted Charged Off): {tp}\")\n",
        "print(f\"\\n>> Insight: The model correctly identifies {tp/(tp+fn)*100:.1f}% of actual defaults (Recall).\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.3 ROC-AUC Curve"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# 5.3.1 ROC-AUC Curve\n",
        "# ============================================================\n",
        "fpr, tpr, thresholds_roc = roc_curve(y_test, y_pred_proba)\n",
        "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "ax.plot(fpr, tpr, color='#e74c3c', linewidth=3, label=f'Logistic Regression (AUC = {roc_auc:.4f})')\n",
        "ax.plot([0, 1], [0, 1], color='gray', linestyle='--', linewidth=2, label='Random Classifier (AUC = 0.5)')\n",
        "ax.fill_between(fpr, tpr, alpha=0.1, color='#e74c3c')\n",
        "\n",
        "# Find optimal threshold (Youden's J statistic)\n",
        "j_scores = tpr - fpr\n",
        "optimal_idx = np.argmax(j_scores)\n",
        "optimal_threshold_roc = thresholds_roc[optimal_idx]\n",
        "ax.scatter(fpr[optimal_idx], tpr[optimal_idx], marker='*', s=300, color='gold', \n",
        "           edgecolor='black', zorder=5, label=f'Optimal Threshold = {optimal_threshold_roc:.3f}')\n",
        "\n",
        "ax.set_title('ROC-AUC Curve', fontsize=18, fontweight='bold')\n",
        "ax.set_xlabel('False Positive Rate (1 - Specificity)', fontsize=14)\n",
        "ax.set_ylabel('True Positive Rate (Sensitivity/Recall)', fontsize=14)\n",
        "ax.legend(fontsize=13, loc='lower right')\n",
        "ax.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig('../figures/roc_auc_curve.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nROC-AUC Score: {roc_auc:.4f}\")\n",
        "print(f\"Optimal Threshold (Youden's J): {optimal_threshold_roc:.4f}\")\n",
        "print(f\"\\n>> Insight: An AUC of {roc_auc:.4f} indicates the model has good discriminatory power.\")\n",
        "print(\"   The model can distinguish between defaulters and non-defaulters significantly better than random.\")\n",
        "print(\"   AUC > 0.7 is generally considered acceptable; > 0.8 is good; > 0.9 is excellent.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.4 Precision-Recall Curve"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# 5.4.1 Precision-Recall Curve\n",
        "# ============================================================\n",
        "precision_curve, recall_curve, thresholds_pr = precision_recall_curve(y_test, y_pred_proba)\n",
        "pr_auc = auc(recall_curve, precision_curve)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "ax.plot(recall_curve, precision_curve, color='#3498db', linewidth=3, label=f'Logistic Regression (PR-AUC = {pr_auc:.4f})')\n",
        "ax.fill_between(recall_curve, precision_curve, alpha=0.1, color='#3498db')\n",
        "\n",
        "# Baseline\n",
        "baseline = y_test.mean()\n",
        "ax.axhline(y=baseline, color='gray', linestyle='--', linewidth=2, label=f'Baseline (Prevalence = {baseline:.3f})')\n",
        "\n",
        "# Find optimal F1 threshold\n",
        "f1_scores = 2 * (precision_curve * recall_curve) / (precision_curve + recall_curve + 1e-8)\n",
        "optimal_f1_idx = np.argmax(f1_scores)\n",
        "ax.scatter(recall_curve[optimal_f1_idx], precision_curve[optimal_f1_idx], \n",
        "           marker='*', s=300, color='gold', edgecolor='black', zorder=5,\n",
        "           label=f'Best F1 Threshold = {thresholds_pr[optimal_f1_idx]:.3f}')\n",
        "\n",
        "ax.set_title('Precision-Recall Curve', fontsize=18, fontweight='bold')\n",
        "ax.set_xlabel('Recall (Sensitivity)', fontsize=14)\n",
        "ax.set_ylabel('Precision', fontsize=14)\n",
        "ax.legend(fontsize=12, loc='upper right')\n",
        "ax.grid(True, alpha=0.3)\n",
        "ax.set_xlim([0, 1])\n",
        "ax.set_ylim([0, 1])\n",
        "plt.tight_layout()\n",
        "plt.savefig('../figures/precision_recall_curve.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nPR-AUC Score: {pr_auc:.4f}\")\n",
        "print(f\"Best F1 Score: {f1_scores[optimal_f1_idx]:.4f} at threshold {thresholds_pr[optimal_f1_idx]:.4f}\")\n",
        "print(f\"\\n>> Insight: The PR curve is particularly important for imbalanced datasets.\")\n",
        "print(f\"   A PR-AUC of {pr_auc:.4f} shows the model's ability to identify defaulters while maintaining precision.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.5 Precision vs Recall Tradeoff Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# 5.5.1 Precision-Recall Tradeoff at Different Thresholds\n",
        "# ============================================================\n",
        "thresholds_analysis = np.arange(0.1, 0.9, 0.05)\n",
        "\n",
        "results = []\n",
        "for thresh in thresholds_analysis:\n",
        "    y_pred_thresh = (y_pred_proba >= thresh).astype(int)\n",
        "    prec = precision_score(y_test, y_pred_thresh, zero_division=0)\n",
        "    rec = recall_score(y_test, y_pred_thresh, zero_division=0)\n",
        "    f1 = f1_score(y_test, y_pred_thresh, zero_division=0)\n",
        "    acc = accuracy_score(y_test, y_pred_thresh)\n",
        "    results.append({'Threshold': thresh, 'Precision': prec, 'Recall': rec, 'F1': f1, 'Accuracy': acc})\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(14, 8))\n",
        "ax.plot(results_df['Threshold'], results_df['Precision'], 'b-o', linewidth=2, markersize=6, label='Precision')\n",
        "ax.plot(results_df['Threshold'], results_df['Recall'], 'r-s', linewidth=2, markersize=6, label='Recall')\n",
        "ax.plot(results_df['Threshold'], results_df['F1'], 'g-^', linewidth=2, markersize=6, label='F1-Score')\n",
        "ax.plot(results_df['Threshold'], results_df['Accuracy'], 'm-d', linewidth=2, markersize=6, label='Accuracy')\n",
        "\n",
        "# Highlight optimal F1 threshold\n",
        "best_f1_idx = results_df['F1'].idxmax()\n",
        "ax.axvline(x=results_df.loc[best_f1_idx, 'Threshold'], color='green', linestyle='--', alpha=0.5, \n",
        "           label=f'Best F1 Threshold = {results_df.loc[best_f1_idx, \"Threshold\"]:.2f}')\n",
        "\n",
        "ax.set_title('Precision-Recall-F1-Accuracy Tradeoff at Different Thresholds', fontsize=16, fontweight='bold')\n",
        "ax.set_xlabel('Classification Threshold', fontsize=14)\n",
        "ax.set_ylabel('Score', fontsize=14)\n",
        "ax.legend(fontsize=12, loc='center left')\n",
        "ax.grid(True, alpha=0.3)\n",
        "ax.set_xlim([0.1, 0.85])\n",
        "plt.tight_layout()\n",
        "plt.savefig('../figures/precision_recall_tradeoff.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nThreshold Analysis Table:\")\n",
        "print(results_df.round(4).to_string(index=False))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 6. Tradeoff Questions & Business Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# 6.1 Tradeoff Question 1: Detecting Real Defaulters with Less False Positives\n",
        "# ============================================================\n",
        "print(\"=\" * 80)\n",
        "print(\"TRADEOFF QUESTION 1: Detecting Real Defaulters with Less False Positives\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\"\"\n",
        "BUSINESS CONTEXT:\n",
        "- False Positives (FP): Predicting a good borrower as defaulter -> LOST BUSINESS OPPORTUNITY\n",
        "- We want to MAXIMIZE RECALL (catch real defaulters) while keeping FP low\n",
        "\n",
        "STRATEGY: Focus on RECALL with acceptable Precision\n",
        "\n",
        "APPROACH:\n",
        "1. Lower the classification threshold to catch more defaults\n",
        "2. Use class_weight='balanced' (already implemented)\n",
        "3. Monitor the precision-recall tradeoff carefully\n",
        "\n",
        "If we want to detect real defaulters while minimizing false positives, we should:\n",
        "- Use a MODERATE threshold that balances both metrics\n",
        "- Focus on F1-Score as the primary metric (harmonic mean of precision & recall)\n",
        "- Consider the COST of each type of error:\n",
        "  * Cost of FP (rejecting good borrower) = Lost interest income\n",
        "  * Cost of FN (approving bad borrower) = Loan amount loss\n",
        "\"\"\")\n",
        "\n",
        "# Scenario 1: High Recall threshold (conservative)\n",
        "high_recall_thresh = 0.3\n",
        "y_pred_hr = (y_pred_proba >= high_recall_thresh).astype(int)\n",
        "print(f\"\\n--- Scenario: Lower Threshold ({high_recall_thresh}) for Higher Recall ---\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred_hr):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred_hr):.4f}\")\n",
        "print(f\"F1-Score: {f1_score(y_test, y_pred_hr):.4f}\")\n",
        "print(f\"False Positives: {((y_pred_hr == 1) & (y_test == 0)).sum()}\")\n",
        "print(f\"False Negatives: {((y_pred_hr == 0) & (y_test == 1)).sum()}\")\n",
        "\n",
        "print(f\"\\n>> ANSWER Q6: From a bank's perspective, the primary focus should be on RECALL.\")\n",
        "print(\"   Reason: Missing a defaulter (False Negative) is MORE COSTLY than rejecting a\")\n",
        "print(\"   good borrower (False Positive). The cost of NPA far exceeds the lost interest income.\")\n",
        "print(\"   However, if the bank wants balanced growth + safety, F1-Score is preferred.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# 6.2 Tradeoff Question 2: NPA Prevention - Play it Safe\n",
        "# ============================================================\n",
        "print(\"=\" * 80)\n",
        "print(\"TRADEOFF QUESTION 2: NPA Prevention - Playing it Safe\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\"\"\n",
        "BUSINESS CONTEXT:\n",
        "- NPA (Non-Performing Asset) is a critical risk for banks\n",
        "- We should NOT disburse loans to potential defaulters\n",
        "- Priority: MAXIMIZE RECALL (catch ALL defaulters, even at the cost of some FP)\n",
        "\n",
        "STRATEGY: Use a LOWER threshold to flag more potential defaults\n",
        "\"\"\")\n",
        "\n",
        "# Conservative approach - very low threshold\n",
        "safe_thresh = 0.25\n",
        "y_pred_safe = (y_pred_proba >= safe_thresh).astype(int)\n",
        "\n",
        "print(f\"--- Conservative Approach: Threshold = {safe_thresh} ---\")\n",
        "print(f\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_safe, target_names=['Fully Paid', 'Charged Off']))\n",
        "\n",
        "cm_safe = confusion_matrix(y_test, y_pred_safe)\n",
        "tn_s, fp_s, fn_s, tp_s = cm_safe.ravel()\n",
        "\n",
        "print(f\"\\nWith threshold = {safe_thresh}:\")\n",
        "print(f\"  Defaulters caught: {tp_s} out of {tp_s + fn_s} ({tp_s/(tp_s+fn_s)*100:.1f}%)\")\n",
        "print(f\"  Good borrowers incorrectly flagged: {fp_s} out of {tn_s + fp_s} ({fp_s/(tn_s+fp_s)*100:.1f}%)\")\n",
        "print(f\"  Missed defaulters (NPA risk): {fn_s}\")\n",
        "\n",
        "# Cost-benefit analysis\n",
        "avg_loan = df['loan_amnt'].mean()\n",
        "avg_interest = df['int_rate'].mean()\n",
        "print(f\"\\n--- Cost-Benefit Analysis ---\")\n",
        "print(f\"  Average loan amount: ${avg_loan:,.0f}\")\n",
        "print(f\"  Average interest rate: {avg_interest:.1f}%\")\n",
        "print(f\"  Estimated NPA savings (caught defaults): ${tp_s * avg_loan:,.0f}\")\n",
        "print(f\"  Estimated lost opportunity (false positives): ${fp_s * avg_loan * avg_interest/100:,.0f}\")\n",
        "print(f\"  Net benefit: ${tp_s * avg_loan - fp_s * avg_loan * avg_interest/100:,.0f}\")\n",
        "\n",
        "print(\"\\n>> Recommendation: Use conservative threshold (0.25-0.30) to minimize NPA risk.\")\n",
        "print(\"   The cost of a bad loan far exceeds the opportunity cost of a missed good loan.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# 6.3 Visualization: Impact of Different Thresholds\n",
        "# ============================================================\n",
        "fig, axes = plt.subplots(1, 3, figsize=(22, 7))\n",
        "\n",
        "threshold_scenarios = [\n",
        "    (0.25, 'Conservative (NPA Safe)', '#e74c3c'),\n",
        "    (0.5, 'Default Threshold', '#3498db'),\n",
        "    (0.65, 'Aggressive (Growth)', '#2ecc71')\n",
        "]\n",
        "\n",
        "for idx, (thresh, title, color) in enumerate(threshold_scenarios):\n",
        "    y_p = (y_pred_proba >= thresh).astype(int)\n",
        "    cm_t = confusion_matrix(y_test, y_p)\n",
        "    sns.heatmap(cm_t, annot=True, fmt='d', cmap='Blues', ax=axes[idx],\n",
        "                xticklabels=['Fully Paid', 'Charged Off'],\n",
        "                yticklabels=['Fully Paid', 'Charged Off'])\n",
        "    prec = precision_score(y_test, y_p, zero_division=0)\n",
        "    rec = recall_score(y_test, y_p, zero_division=0)\n",
        "    axes[idx].set_title(f'{title}\\nThreshold={thresh} | P={prec:.2f} | R={rec:.2f}', \n",
        "                        fontsize=13, fontweight='bold', color=color)\n",
        "    axes[idx].set_xlabel('Predicted')\n",
        "    axes[idx].set_ylabel('Actual')\n",
        "\n",
        "plt.suptitle('Confusion Matrices at Different Thresholds', fontsize=16, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.savefig('../figures/threshold_comparison.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n>> ANSWER Q7: Gap in Precision and Recall affects the bank as follows:\")\n",
        "print(\"   - High Precision, Low Recall: Bank misses many defaulters -> HIGH NPA RISK\")\n",
        "print(\"   - High Recall, Low Precision: Bank rejects many good borrowers -> LOST REVENUE\")\n",
        "print(\"   - The gap indicates the model struggles to simultaneously identify all defaults\")\n",
        "print(\"     while maintaining accuracy in predictions.\")\n",
        "print(\"   - A large gap forces the bank to choose: safety (recall) vs growth (precision)\")\n",
        "print(\"   - For loan defaults, RECALL should be prioritized as NPA cost >> opportunity cost\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 7. Actionable Insights & Recommendations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# 7.1 Summary of All Findings\n",
        "# ============================================================\n",
        "print(\"=\" * 80)\n",
        "print(\"               ACTIONABLE INSIGHTS & RECOMMENDATIONS\")\n",
        "print(\"               Author: Vidyasagar | Data Scientist\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "fully_paid_pct = (df['loan_status'] == 'Fully Paid').mean() * 100\n",
        "top2_titles = df['emp_title'].value_counts().head(2).index.tolist()\n",
        "majority_home = df['home_ownership'].value_counts().index[0]\n",
        "\n",
        "print(f\"\"\"\n",
        "============================================================\n",
        "QUESTIONNAIRE ANSWERS\n",
        "============================================================\n",
        "\n",
        "Q1. What percentage of customers have fully paid their Loan Amount?\n",
        "    ANSWER: {fully_paid_pct:.2f}% of customers have fully paid their loan.\n",
        "\n",
        "Q2. Comment about the correlation between Loan Amount and Installment features.\n",
        "    ANSWER: There is a very strong positive correlation ({df['loan_amnt'].corr(df['installment']):.4f})\n",
        "    between Loan Amount and Installment. This is expected because the monthly installment\n",
        "    is directly calculated from the loan amount, interest rate, and term. This indicates\n",
        "    potential multicollinearity, and one of these features could be dropped during modeling.\n",
        "\n",
        "Q3. The majority of people have home ownership as _______.\n",
        "    ANSWER: The majority of people have home ownership as '{majority_home}'.\n",
        "\n",
        "Q4. People with grades 'A' are more likely to fully pay their loan. (T/F)\n",
        "    ANSWER: TRUE. Grade A borrowers have the highest repayment rate among all grades,\n",
        "    with significantly lower default rates compared to grades D, E, F, and G.\n",
        "\n",
        "Q5. Name the top 2 afforded job titles.\n",
        "    ANSWER: The top 2 job titles are '{top2_titles[0]}' and '{top2_titles[1]}'.\n",
        "\n",
        "Q6. Thinking from a bank's perspective, which metric should our primary focus be on?\n",
        "    ANSWER: RECALL should be the primary focus. In the lending business, the cost of\n",
        "    approving a defaulter (False Negative) far exceeds the cost of rejecting a good\n",
        "    borrower (False Positive). NPA directly impacts the bank's balance sheet, while\n",
        "    a rejected good borrower is merely a missed opportunity. However, for a balanced\n",
        "    approach, F1-Score can be used as it considers both precision and recall.\n",
        "\n",
        "Q7. How does the gap in precision and recall affect the bank?\n",
        "    ANSWER: The precision-recall gap forces the bank into a strategic tradeoff:\n",
        "    - If precision >> recall: Many defaulters are missed, leading to HIGH NPA.\n",
        "    - If recall >> precision: Many good borrowers rejected, leading to LOST REVENUE.\n",
        "    The bank must choose based on its risk appetite. A conservative bank should\n",
        "    prioritize recall; a growth-oriented bank may tolerate more risk for higher revenue.\n",
        "\n",
        "Q8. Which were the features that heavily affected the outcome?\n",
        "    ANSWER: The top features affecting loan default prediction are:\"\"\")\n",
        "\n",
        "for i, row in coefficients.head(5).iterrows():\n",
        "    direction = 'INCREASES' if row['Coefficient'] > 0 else 'DECREASES'\n",
        "    print(f\"    - {row['Feature']} (coef: {row['Coefficient']:.4f}) -> {direction} default risk\")\n",
        "\n",
        "print(f\"\"\"\n",
        "Q9. Will the results be affected by geographical location? (Yes/No)\n",
        "    ANSWER: YES. Analysis shows default rates vary significantly across states.\n",
        "    Economic conditions, employment rates, and cost of living differ by region,\n",
        "    all of which influence a borrower's ability to repay loans.\n",
        "\n",
        "============================================================\n",
        "KEY BUSINESS INSIGHTS\n",
        "============================================================\n",
        "\n",
        "1. RISK SEGMENTATION BY GRADE:\n",
        "   - Grade A-B borrowers have <15% default rate -> LOW RISK -> Favorable terms\n",
        "   - Grade D-G borrowers have >30% default rate -> HIGH RISK -> Stricter screening\n",
        "\n",
        "2. TERM EFFECT:\n",
        "   - 60-month loans have significantly higher default rates than 36-month\n",
        "   - Recommendation: Incentivize shorter loan terms\n",
        "\n",
        "3. INTEREST RATE PARADOX:\n",
        "   - Higher interest rates correlate with higher defaults\n",
        "   - Risk-based pricing must be carefully calibrated\n",
        "\n",
        "4. EMPLOYMENT STABILITY:\n",
        "   - Borrowers with 10+ years employment have lower default rates\n",
        "   - Employment length is a strong predictor of repayment capability\n",
        "\n",
        "5. DTI RATIO THRESHOLD:\n",
        "   - Borrowers with DTI > 25 show significantly higher default rates\n",
        "   - Consider DTI caps for loan approval\n",
        "\n",
        "============================================================\n",
        "RECOMMENDATIONS\n",
        "============================================================\n",
        "\n",
        "1. IMPLEMENT TIERED APPROVAL:\n",
        "   - Automatic approval for low-risk (Grade A-B, low DTI, stable employment)\n",
        "   - Enhanced review for medium-risk (Grade C-D)\n",
        "   - Additional collateral/guarantor for high-risk (Grade E-G)\n",
        "\n",
        "2. THRESHOLD OPTIMIZATION:\n",
        "   - Use threshold of 0.25-0.30 for conservative lending (NPA minimization)\n",
        "   - Use threshold of 0.45-0.50 for balanced growth\n",
        "\n",
        "3. FEATURE MONITORING:\n",
        "   - Continuously monitor top predictive features\n",
        "   - Implement early warning system based on DTI changes\n",
        "\n",
        "4. GEOGRAPHIC RISK ADJUSTMENT:\n",
        "   - Apply state-level risk factors to loan pricing\n",
        "   - Monitor regional economic indicators\n",
        "\n",
        "5. MODEL IMPROVEMENT:\n",
        "   - Consider ensemble methods (Random Forest, XGBoost) for better performance\n",
        "   - Implement SMOTE or other oversampling techniques for class imbalance\n",
        "   - Regular model retraining with updated data\n",
        "\n",
        "ROC-AUC Score: {roc_auc:.4f}\n",
        "The model demonstrates strong discriminatory power and can be deployed\n",
        "as part of the loan approval pipeline with appropriate threshold tuning.\n",
        "\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# 7.2 Final Summary Visualization\n",
        "# ============================================================\n",
        "fig, axes = plt.subplots(2, 2, figsize=(18, 14))\n",
        "\n",
        "# 1. Default Rate by Grade\n",
        "grade_default = df.groupby('grade')['loan_status'].apply(lambda x: (x == 'Charged Off').mean() * 100)\n",
        "grade_default.plot(kind='bar', ax=axes[0, 0], color=sns.color_palette('RdYlGn_r', 7), edgecolor='black')\n",
        "axes[0, 0].set_title('Default Rate by Grade', fontsize=14, fontweight='bold')\n",
        "axes[0, 0].set_ylabel('Default Rate (%)')\n",
        "axes[0, 0].tick_params(axis='x', rotation=0)\n",
        "for p in axes[0, 0].patches:\n",
        "    axes[0, 0].annotate(f'{p.get_height():.1f}%', \n",
        "                        (p.get_x() + p.get_width()/2., p.get_height()),\n",
        "                        ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# 2. Default Rate by Term\n",
        "term_default = df.groupby('term')['loan_status'].apply(lambda x: (x == 'Charged Off').mean() * 100)\n",
        "term_default.plot(kind='bar', ax=axes[0, 1], color=['#2ecc71', '#e74c3c'], edgecolor='black')\n",
        "axes[0, 1].set_title('Default Rate by Term', fontsize=14, fontweight='bold')\n",
        "axes[0, 1].set_ylabel('Default Rate (%)')\n",
        "axes[0, 1].tick_params(axis='x', rotation=0)\n",
        "for p in axes[0, 1].patches:\n",
        "    axes[0, 1].annotate(f'{p.get_height():.1f}%', \n",
        "                        (p.get_x() + p.get_width()/2., p.get_height()),\n",
        "                        ha='center', va='bottom', fontweight='bold')\n",
        "\n",
        "# 3. ROC Curve\n",
        "axes[1, 0].plot(fpr, tpr, color='#e74c3c', linewidth=3, label=f'AUC = {roc_auc:.4f}')\n",
        "axes[1, 0].plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
        "axes[1, 0].fill_between(fpr, tpr, alpha=0.1, color='#e74c3c')\n",
        "axes[1, 0].set_title('ROC-AUC Curve', fontsize=14, fontweight='bold')\n",
        "axes[1, 0].set_xlabel('False Positive Rate')\n",
        "axes[1, 0].set_ylabel('True Positive Rate')\n",
        "axes[1, 0].legend(fontsize=12)\n",
        "\n",
        "# 4. Feature Importance (Top 10)\n",
        "top10 = coefficients.head(10)\n",
        "colors = ['#e74c3c' if c > 0 else '#2ecc71' for c in top10['Coefficient']]\n",
        "axes[1, 1].barh(top10['Feature'], top10['Coefficient'], color=colors, edgecolor='black')\n",
        "axes[1, 1].set_title('Top 10 Feature Importance', fontsize=14, fontweight='bold')\n",
        "axes[1, 1].set_xlabel('Coefficient')\n",
        "axes[1, 1].axvline(x=0, color='black', linewidth=1)\n",
        "\n",
        "plt.suptitle('Loan Default Prediction - Executive Summary\\nby Vidyasagar, Data Scientist', \n",
        "             fontsize=18, fontweight='bold', y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.savefig('../figures/executive_summary.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"All figures saved to ../figures/ directory\")\n",
        "print(\"=\" * 60)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# Final Model Performance Summary\n",
        "# ============================================================\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"         FINAL MODEL PERFORMANCE SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"\"\"  \n",
        "  Model: Logistic Regression (class_weight='balanced')\n",
        "  \n",
        "  Training Samples: {X_train.shape[0]}\n",
        "  Test Samples: {X_test.shape[0]}\n",
        "  Number of Features: {X_train.shape[1]}\n",
        "  \n",
        "  ---- Default Threshold (0.5) ----\n",
        "  Accuracy:  {accuracy_score(y_test, y_pred):.4f}\n",
        "  Precision: {precision_score(y_test, y_pred):.4f}\n",
        "  Recall:    {recall_score(y_test, y_pred):.4f}\n",
        "  F1-Score:  {f1_score(y_test, y_pred):.4f}\n",
        "  ROC-AUC:   {roc_auc:.4f}\n",
        "  PR-AUC:    {pr_auc:.4f}\n",
        "  \n",
        "  ---- Recommended Threshold (0.30) ----\n",
        "  Precision: {precision_score(y_test, (y_pred_proba >= 0.30).astype(int)):.4f}\n",
        "  Recall:    {recall_score(y_test, (y_pred_proba >= 0.30).astype(int)):.4f}\n",
        "  F1-Score:  {f1_score(y_test, (y_pred_proba >= 0.30).astype(int)):.4f}\n",
        "  \n",
        "  Conclusion: The model performs well for identifying loan defaults.\n",
        "  Use threshold tuning based on business risk appetite.\n",
        "\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}